"""
example1 Example - Load .inp, run SWMM simulation, and convert to JSON and Parquet

This example demonstrates:
1. Loading a SWMM .inp file
2. Running the SWMM simulation engine
3. Outputting to JSON format
4. Outputting to Parquet format (both single-file and multi-file modes)
5. Parsing the SWMM report (.rpt) file
6. Parsing the SWMM output (.out) binary file
"""

import subprocess
from pathlib import Path
from swmm_utils import SwmmInput, SwmmReport, SwmmOutput


def main():
    """Load, simulate, and convert SWMM model files to various formats."""
    # Setup paths
    example_dir = Path(__file__).parent
    input_file = example_dir / "example1.inp"
    report_file = example_dir / "example1.rpt"
    output_file = example_dir / "example1.out"
    output_dir = example_dir  # Output to the same folder as the script

    # Path to runswmm executable
    bin_dir = example_dir.parent.parent / "bin"
    runswmm = bin_dir / "runswmm"

    print("=" * 80)
    print("example1 Example - Load, Simulate, and Convert SWMM Model")
    print("=" * 80)

    # Load the SWMM input file
    print(f"\nðŸ“– Loading SWMM file: {input_file.name}")

    with SwmmInput(input_file) as inp:
        print("   âœ“ Successfully loaded!")
        print(f"   âœ“ Model title: {inp.title}")
        print(f"   âœ“ Model contains {len(list(inp.keys()))} sections")

        # Show model statistics
        if inp.subcatchments:
            print(f"   âœ“ Subcatchments: {len(inp.subcatchments)}")
        if inp.junctions:
            print(f"   âœ“ Junctions: {len(inp.junctions)}")
        if inp.outfalls:
            print(f"   âœ“ Outfalls: {len(inp.outfalls)}")
        if inp.conduits:
            print(f"   âœ“ Conduits: {len(inp.conduits)}")

        # Output to JSON
        print("ðŸ’¾ Saving to JSON format")
        json_output = output_dir / "example1.inp.json"
        inp.to_json(json_output, pretty=True)

        if json_output.exists():
            size = json_output.stat().st_size
            print(f"   âœ“ Saved: {json_output}")
            print(f"   âœ“ Size: {size:,} bytes")

        # Output to Parquet (multi-file mode)
        print("ðŸ’¾ Saving to Parquet format (multi-file)")
        parquet_dir = output_dir / "example1.inp_parquet"
        inp.to_parquet(parquet_dir, single_file=False)

        if parquet_dir.exists():
            parquet_files = list(parquet_dir.glob("*.parquet"))
            total_size = sum(f.stat().st_size for f in parquet_files)
            print(f"   âœ“ Saved: {parquet_dir}/")
            print(f"   âœ“ Files created: {len(parquet_files)}")
            print(f"   âœ“ Total size: {total_size:,} bytes")

        # Output to Parquet (single-file mode)
        print("ðŸ’¾ Saving to Parquet format (single-file)")
        parquet_file = output_dir / "example1.inp.parquet"
        inp.to_parquet(parquet_file, single_file=True)

        if parquet_file.exists():
            size = parquet_file.stat().st_size
            print(f"   âœ“ Saved: {parquet_file}")
            print(f"   âœ“ Size: {size:,} bytes")

        # Export to Pandas DataFrames
        print("ðŸ“Š Exporting to Pandas DataFrames")
        try:
            # Export all sections to DataFrames
            print("\n   ðŸ“‹ All Sections as DataFrames:")
            all_dfs = inp.to_dataframe()

            # Export specific section to DataFrame
            if inp.junctions and "junctions" in all_dfs:
                print("\n   ðŸ“‹ Junctions DataFrame:")
                junctions_df = all_dfs["junctions"]
                print(f"      âœ“ Rows: {len(junctions_df)}")
                print(f"      âœ“ Columns: {list(junctions_df.columns)}")
                print("      Sample data:")
                print(junctions_df.head(3).to_string(index=False))
            print(f"      âœ“ Sections available: {list(all_dfs.keys())}")

            # Access and manipulate specific dataframes
            if "conduits" in all_dfs and len(all_dfs["conduits"]) > 0:
                print("\n   ðŸ“‹ Conduits DataFrame:")
                conduits_df = all_dfs["conduits"]
                print(f"      âœ“ Rows: {len(conduits_df)}")
                print(f"      âœ“ Columns: {list(conduits_df.columns)}")
                print("      Sample data:")
                print(conduits_df.head(3).to_string(index=False))

                # Example: Get statistics on numeric columns
                numeric_cols = conduits_df.select_dtypes(include=["number"]).columns
                if len(numeric_cols) > 0:
                    print(f"\n      Numeric columns: {list(numeric_cols)}")
                    if "length" in numeric_cols:
                        print(
                            f"      Length statistics: min={conduits_df['length'].min():.0f}, "
                            f"mean={conduits_df['length'].mean():.0f}, "
                            f"max={conduits_df['length'].max():.0f}"
                        )

            # Export subcatchments and calculate statistics
            if "subcatchments" in all_dfs and len(all_dfs["subcatchments"]) > 0:
                print("\n   ðŸ“‹ Subcatchments DataFrame:")
                subs_df = all_dfs["subcatchments"]
                print(f"      âœ“ Rows: {len(subs_df)}")
                print(f"      âœ“ Columns: {list(subs_df.columns)}")

                # Show numeric column statistics
                numeric_cols = subs_df.select_dtypes(include=["number"]).columns
                if len(numeric_cols) > 0:
                    print(f"      âœ“ Numeric columns: {list(numeric_cols)}")
                    for col in numeric_cols[:2]:  # Show first 2 numeric columns
                        print(
                            f"      {col}: min={subs_df[col].min()}, "
                            f"mean={subs_df[col].mean():.2f}, max={subs_df[col].max()}"
                        )

        except ImportError:
            print("   âš  pandas not installed, skipping DataFrame export")

    # Run SWMM simulation
    print("ðŸš€ Running SWMM simulation")
    print(f"   Input: {input_file.name}")
    print(f"   Report: {report_file.name}")
    print(f"   Output: {output_file.name}")

    try:
        subprocess.run(
            [str(runswmm), str(input_file), str(report_file), str(output_file)],
            capture_output=True,
            text=True,
            check=True,
        )
        print("   âœ“ Simulation completed successfully!")

        if report_file.exists():
            rpt_size = report_file.stat().st_size
            print(f"   âœ“ Report generated: {report_file.name} ({rpt_size:,} bytes)")

        if output_file.exists():
            out_size = output_file.stat().st_size
            print(f"   âœ“ Output generated: {output_file.name} ({out_size:,} bytes)")

    except subprocess.CalledProcessError as e:
        print(f"   âœ— Simulation failed with exit code {e.returncode}")
        if e.stderr:
            print(f"   Error: {e.stderr}")
    except FileNotFoundError:
        print(f"   âœ— Could not find runswmm executable at {runswmm}")

    # Parse report file
    if report_file.exists():
        print("ðŸ“Š Parsing SWMM report file")
        with SwmmReport(report_file) as report:
            print("   âœ“ Successfully parsed!")
            print(f"   âœ“ SWMM Version: {report.header.get('version', 'Unknown')}")
            print("   âœ“ Analysis Options:")
            print(
                f"      - Flow Units: {report.analysis_options.get('flow_units', 'N/A')}"
            )
            print(
                f"      - Routing Method: {report.analysis_options.get('flow_routing_method', 'N/A')}"
            )

            # Show node depth summary
            if report.node_depth:
                print("\n   ðŸ’§ Node Depth Summary:")
                for node in report.node_depth:
                    print(
                        f"      {node['name']} ({node['type']}): {node['maximum_depth']:.2f} ft max depth"
                    )

            # Show link flow summary
            if report.link_flow:
                print("\n   ðŸŒŠ Link Flow Summary:")
                for link in report.link_flow:
                    print(
                        f"      {link['name']}: {link['maximum_flow']:.2f} CFS max flow, {link['maximum_velocity']:.2f} ft/s max velocity"
                    )

    # Parse output file
    if output_file.exists():
        print("ðŸ’¾ Parsing SWMM output file")
        try:
            # Approach 1: Load metadata only (default behavior, fast and lightweight)
            out = SwmmOutput(output_file)
            print("   âœ“ Successfully parsed!")
            print(f"   âœ“ SWMM Version: {out.version}")
            print(f"   âœ“ Flow Unit: {out.flow_unit}")
            print(
                f"   âœ“ Simulation Period: {out.start_date.strftime('%Y-%m-%d %H:%M:%S')} to {out.end_date.strftime('%Y-%m-%d %H:%M:%S')}"
            )
            print(f"   âœ“ Report Interval: {out.report_interval}")
            print(f"   âœ“ Time Steps: {out.n_periods}")

            # Show model summary
            summary = out.summary()
            print("\n   ðŸ“‹ Model Summary:")
            print(f"      - Subcatchments: {summary['n_subcatchments']}")
            print(f"      - Nodes: {summary['n_nodes']}")
            print(f"      - Links: {summary['n_links']}")
            print(f"      - Pollutants: {summary['n_pollutants']}")
            if summary["pollutants"]:
                print(f"      - Pollutant List: {', '.join(summary['pollutants'])}")

            # Export output to JSON (metadata only)
            print("\n   ðŸ’¾ Saving output metadata to JSON format")
            out_json = output_dir / "example1.out.json"
            out.to_json(out_json, pretty=True)
            if out_json.exists():
                size = out_json.stat().st_size
                print(f"      âœ“ Saved: {out_json.name} ({size:,} bytes)")

            # Approach 2: Load with full time series data (for comprehensive analysis)
            print("\n   ðŸ’¾ Loading output file with full time series data")
            out_with_ts = SwmmOutput(output_file, load_time_series=True)
            print(
                f"      âœ“ Loaded with time series data from {out_with_ts.n_periods} time steps"
            )

            # Export output to JSON (with full time series)
            print("\n   ðŸ’¾ Saving output with full time series to JSON format")
            out_json_ts = output_dir / "example1.out_with_timeseries.json"
            out_with_ts.to_json(out_json_ts, pretty=True)
            if out_json_ts.exists():
                size_ts = out_json_ts.stat().st_size
                print(f"      âœ“ Saved: {out_json_ts.name} ({size_ts:,} bytes)")
                if out_json.exists():
                    ratio = size_ts / size
                    print(
                        f"      â„¹ï¸  Size comparison: {ratio:.1f}x larger than metadata-only export"
                    )

            # Export output to Parquet (single file)
            print("\n   ðŸ’¾ Saving output metadata to Parquet format (single-file)")
            out_parquet = output_dir / "example1.out.parquet"
            try:
                out.to_parquet(out_parquet, single_file=True)
                if out_parquet.exists():
                    size = out_parquet.stat().st_size
                    print(f"      âœ“ Saved: {out_parquet.name} ({size:,} bytes)")
            except ImportError:
                print("      âš  pandas/pyarrow not installed, skipping Parquet export")

            # Export output to Parquet (multi-file)
            print("\n   ðŸ’¾ Saving output metadata to Parquet format (multi-file)")
            out_parquet_dir = output_dir / "example1.out_parquet"
            try:
                out.to_parquet(out_parquet_dir, single_file=False)
                if out_parquet_dir.exists():
                    parquet_files = list(out_parquet_dir.glob("*.parquet"))
                    total_size = sum(f.stat().st_size for f in parquet_files)
                    print(f"      âœ“ Saved: {out_parquet_dir.name}/")
                    print(f"      âœ“ Files created: {len(parquet_files)}")
                    print(f"      âœ“ Total size: {total_size:,} bytes")
            except ImportError:
                print("      âš  pandas/pyarrow not installed, skipping Parquet export")

            # Export to Pandas DataFrames
            print("\n   ðŸ“Š Exporting time series data to Pandas DataFrames")
            try:
                # Full export with metadata and all sections
                print("\n      Level 1: Full export (all sections + metadata)")
                all_dfs = out_with_ts.to_dataframe()
                print(f"      âœ“ Returned dict with keys: {list(all_dfs.keys())}")
                if "metadata" in all_dfs:
                    meta_df = all_dfs["metadata"]
                    print(
                        f"      âœ“ Metadata: {meta_df.shape[0]} row(s), {meta_df.shape[1]} column(s)"
                    )
                if "links" in all_dfs and len(all_dfs["links"]) > 0:
                    links_df = all_dfs["links"]
                    print(
                        f"      âœ“ Links: {links_df.shape[0]} row(s) ({out_with_ts.n_links} links Ã— {out_with_ts.n_periods} periods)"
                    )

                # Section-level export (links only)
                print("\n      Level 2: Section export (links only)")
                links_section_data = out_with_ts.to_dataframe("links")
                links_section_df = links_section_data.get("links")
                if links_section_df is not None:
                    print(
                        f"      âœ“ Links section: {links_section_df.shape[0]} row(s), {links_section_df.shape[1]} column(s)"
                    )
                    if len(links_section_df) > 0:
                        print(
                            f"      âœ“ Index levels: {links_section_df.index.names}"
                        )
                        print("      Sample data (first 3 rows):")
                        print(links_section_df.head(3).to_string())

                # Single-element export (first link)
                if out_with_ts.n_links > 0:
                    first_link = out_with_ts.link_labels[0]
                    print(f"\n      Level 3: Single element export ({first_link})")
                    link_ts_data = out_with_ts.to_dataframe("links", first_link)
                    link_ts_df = link_ts_data.get("links")
                    if link_ts_df is not None:
                        print(
                            f"      âœ“ Single link time series: {link_ts_df.shape[0]} row(s), {link_ts_df.shape[1]} column(s)"
                        )
                        print(f"      âœ“ Time period: {link_ts_df.index.min()} to {link_ts_df.index.max()}")
                        print("      Sample data (first 3 timesteps):")
                        print(link_ts_df.head(3).to_string())

            except ImportError:
                print("      âš  pandas not installed, skipping DataFrame export")

        except (OSError, ValueError) as e:
            print(f"   âœ— Failed to parse output file: {e}")

    # Summary
    print("\n" + "=" * 80)
    print("âœ… Example completed successfully!")
    print("=" * 80)
    print(f"\nOutput directory: {output_dir}")
    print("\nGenerated files:")
    for output_file_path in sorted(output_dir.rglob("*")):
        if output_file_path.is_file():
            size = output_file_path.stat().st_size
            rel_path = output_file_path.relative_to(output_dir)
            print(f"  â€¢ {rel_path} ({size:,} bytes)")
    print()


if __name__ == "__main__":
    main()
